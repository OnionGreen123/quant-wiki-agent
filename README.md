# é‡åŒ–ç™¾ç§‘æ™ºèƒ½ä»£ç† / Quant Wiki Agent

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

---

## ä¸­æ–‡

ä¸€ä¸ªåŸºäºå¤§è¯­è¨€æ¨¡å‹çš„é‡åŒ–ç™¾ç§‘æ–‡æ¡£å¤„ç†å·¥å…·ï¼Œä¸“é—¨ç”¨äºæ‰¹é‡å¤„ç†å’Œç¿»è¯‘markdownæ–‡æ¡£ã€‚

## é¡¹ç›®ç‰¹ç‚¹

- ğŸš€ **é«˜æ•ˆæ‰¹å¤„ç†**ï¼šæ”¯æŒå¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†ï¼Œæœ€å¤§åŒ–å¤„ç†æ•ˆç‡
- ğŸ”§ **æ¨¡å‹å…¼å®¹**ï¼šæ”¯æŒOpenAIå…¼å®¹çš„APIæ¥å£ï¼Œå¯æ¥å…¥å¤šç§å¤§è¯­è¨€æ¨¡å‹
- ğŸ“ **æ™ºèƒ½ç¼–è¾‘**ï¼šåŸºäºYAMLé…ç½®çš„æç¤ºè¯æ¨¡æ¿ï¼Œå¯çµæ´»å®šåˆ¶å¤„ç†é€»è¾‘
- ğŸŒ **æ–‡æ¡£ç¿»è¯‘**ï¼šä¸“é—¨ä¼˜åŒ–çš„ä¸­æ–‡ç¿»è¯‘å’Œæ ¼å¼åŒ–åŠŸèƒ½
- ğŸ“ **ä¿æŒç»“æ„**ï¼šè‡ªåŠ¨ä¿æŒåŸæœ‰æ–‡ä»¶å¤¹ç»“æ„ï¼Œå¤åˆ¶émarkdownæ–‡ä»¶

## é¡¹ç›®ç»“æ„

```
quant-wiki-agent/
â”œâ”€â”€ llm_utils/          # LLMå·¥å…·æ¨¡å—
â”‚   â””â”€â”€ llm_client.py   # å¤§è¯­è¨€æ¨¡å‹å®¢æˆ·ç«¯
â”œâ”€â”€ prompts/            # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ complete_library.yaml
â”‚   â”œâ”€â”€ edit_prompt.yaml
â”‚   â””â”€â”€ re-translate.yaml
â”œâ”€â”€ test/               # æµ‹è¯•å’Œç¤ºä¾‹
â”‚   â”œâ”€â”€ example_usage.py
â”‚   â”œâ”€â”€ folder_processor.py
â”‚   â””â”€â”€ single_file_test.py
â””â”€â”€ README.md
```

## ä¸»è¦åŠŸèƒ½

### 1. LLMå®¢æˆ·ç«¯ (`llm_utils/llm_client.py`)

æä¾›ç»Ÿä¸€çš„å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œæ”¯æŒï¼š
- OpenAI API
- Gemini APIï¼ˆé€šè¿‡OpenAIå…¼å®¹æ¥å£ï¼‰
- å…¶ä»–å…¼å®¹OpenAI APIçš„æ¨¡å‹æœåŠ¡

**åŠŸèƒ½ç‰¹è‰²ï¼š**
- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- ç¯å¢ƒå˜é‡é…ç½®
- ç³»ç»Ÿæç¤ºè¯è®¾ç½®
- é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

### 2. æ‰¹é‡æ–‡æ¡£å¤„ç† (`test/folder_processor.py`)

æ ¸å¿ƒåŠŸèƒ½æ¨¡å—ï¼Œæ”¯æŒï¼š
- å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†markdownæ–‡ä»¶
- è‡ªåŠ¨å¤åˆ¶émarkdownæ–‡ä»¶
- ä¿æŒåŸæœ‰æ–‡ä»¶å¤¹ç»“æ„
- è¿›åº¦ç›‘æ§å’ŒçŠ¶æ€æŠ¥å‘Š

**å¤„ç†æµç¨‹ï¼š**
1. æ‰«æè¾“å…¥æ–‡ä»¶å¤¹ï¼Œè¯†åˆ«markdownæ–‡ä»¶
2. å¤åˆ¶æ‰€æœ‰émarkdownæ–‡ä»¶åˆ°è¾“å‡ºç›®å½•
3. å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†markdownæ–‡ä»¶
4. ç”Ÿæˆå¤„ç†æŠ¥å‘Š

### 3. æç¤ºè¯æ¨¡æ¿ (`prompts/`)

æä¾›ä¸“ä¸šçš„æ–‡æ¡£å¤„ç†æç¤ºè¯ï¼š
- `re-translate.yaml`: ä¸“é—¨ç”¨äºé‡æ–°ç¿»è¯‘å’Œæ ¼å¼åŒ–ä¸­æ–‡æ–‡æ¡£
- `edit_prompt.yaml`: é€šç”¨æ–‡æ¡£ç¼–è¾‘æç¤ºè¯
- `complete_library.yaml`: å®Œæ•´çš„æ–‡æ¡£åº“å¤„ç†æ¨¡æ¿

## å®‰è£…å’Œé…ç½®

### 1. ç¯å¢ƒè¦æ±‚

- Python 3.7+
- ä¾èµ–åŒ…ï¼š
  ```bash
  pip install openai python-dotenv
  ```

### 2. ç¯å¢ƒé…ç½®

åˆ›å»º `.env` æ–‡ä»¶å¹¶é…ç½®ä»¥ä¸‹å˜é‡ï¼š

```env
# æ¨¡å‹é…ç½®
MODEL_NAME=gpt-4o
API_KEY=your_api_key_here
BASE_URL=https://api.openai.com/v1/

# ç³»ç»Ÿæç¤ºè¯ï¼ˆå¯é€‰ï¼‰
DEFAULT_SYSTEM_PROMPT=ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£å¤„ç†åŠ©æ‰‹ã€‚
```

### 3. æ”¯æŒçš„æ¨¡å‹é…ç½®

#### OpenAI æ¨¡å‹
```env
MODEL_NAME=gpt-4o
API_KEY=your_openai_api_key
BASE_URL=https://api.openai.com/v1/
```

#### Gemini æ¨¡å‹
```env
MODEL_NAME=gemini-1.5-pro-latest
API_KEY=your_gemini_api_key
BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
```

## ä½¿ç”¨æ–¹æ³•

### 1. åŸºæœ¬ä½¿ç”¨

```python
from test.folder_processor import process_folder

# å¤„ç†æ–‡ä»¶å¤¹
result = process_folder(
    input_folder_path="/path/to/input",
    output_folder_path="/path/to/output",
    prompt_file_path="/path/to/prompts/re-translate.yaml",
    max_workers=30,
    rate_limit_delay=0.5
)

print(f"å¤„ç†å®Œæˆï¼æˆåŠŸï¼š{result['successful_count']}ï¼Œå¤±è´¥ï¼š{result['failed_count']}")
```

### 2. å•ç‹¬ä½¿ç”¨LLMå®¢æˆ·ç«¯

```python
from llm_utils.llm_client import LLMClient

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = LLMClient(
    model_name="gpt-4o",
    api_key="your_api_key",
    system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç¿»è¯‘åŠ©æ‰‹ã€‚"
)

# è°ƒç”¨æ¨¡å‹
response = client.call("è¯·ç¿»è¯‘è¿™æ®µæ–‡æœ¬")
print(response)
```

### 3. è¿è¡Œç¤ºä¾‹

```bash
cd test
python example_usage.py
```

## é…ç½®å‚æ•°

### æ–‡ä»¶å¤¹å¤„ç†å™¨å‚æ•°

- `input_folder_path`: è¾“å…¥æ–‡ä»¶å¤¹è·¯å¾„
- `output_folder_path`: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
- `prompt_file_path`: æç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨edit_prompt.yamlï¼‰
- `max_workers`: æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤30ï¼‰
- `rate_limit_delay`: APIè°ƒç”¨é—´éš”ç§’æ•°ï¼ˆé»˜è®¤0.5ç§’ï¼‰

### LLMå®¢æˆ·ç«¯å‚æ•°

- `model_name`: æ¨¡å‹åç§°
- `api_key`: APIå¯†é’¥
- `base_url`: APIåŸºç¡€URL
- `system_prompt`: ç³»ç»Ÿæç¤ºè¯
- `max_retries`: æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤3ï¼‰
- `retry_delay`: é‡è¯•é—´éš”ï¼ˆé»˜è®¤1ç§’ï¼‰

## ç‰¹è‰²åŠŸèƒ½

### æ–‡æ¡£æ ¼å¼åŒ–å’Œç¿»è¯‘

ä¸“é—¨é’ˆå¯¹é‡åŒ–é‡‘èæ–‡æ¡£çš„å¤„ç†ï¼Œæ”¯æŒï¼š
- åˆ é™¤å¤šä½™çš„å…ƒæ•°æ®å­—æ®µ
- æ¸…ç†è¶…é“¾æ¥æ ¼å¼
- ç»Ÿä¸€markdownæ ¼å¼
- ä¸­è‹±æ–‡ç¿»è¯‘
- ä¿æŒä»£ç å—å’Œæ•°å­¦å…¬å¼æ ¼å¼

### æ™ºèƒ½é”™è¯¯å¤„ç†

- è‡ªåŠ¨é‡è¯•æœºåˆ¶
- è¯¦ç»†é”™è¯¯æ—¥å¿—
- å¤„ç†è¿›åº¦ç›‘æ§
- å¤±è´¥æ–‡ä»¶ç»Ÿè®¡

### æ€§èƒ½ä¼˜åŒ–

- å¤šçº¿ç¨‹å¹¶è¡Œå¤„ç†
- å¯é…ç½®çš„APIè°ƒç”¨é¢‘ç‡é™åˆ¶
- å†…å­˜å‹å¥½çš„æµå¼å¤„ç†
- è‡ªåŠ¨å¤åˆ¶éå¤„ç†æ–‡ä»¶

## æ³¨æ„äº‹é¡¹

1. **APIé…é¢ç®¡ç†**ï¼šæ³¨æ„æ§åˆ¶å¹¶å‘æ•°é‡å’Œè°ƒç”¨é¢‘ç‡ï¼Œé¿å…è¶…å‡ºAPIé™åˆ¶
2. **æ–‡ä»¶å¤‡ä»½**ï¼šå¤„ç†å‰è¯·å¤‡ä»½åŸå§‹æ–‡ä»¶
3. **æ¨¡å‹é€‰æ‹©**ï¼šæ ¹æ®æ–‡æ¡£å¤æ‚åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
4. **ç½‘ç»œç¨³å®šæ€§**ï¼šç¡®ä¿ç½‘ç»œè¿æ¥ç¨³å®šï¼Œé¿å…å¤„ç†ä¸­æ–­

## è®¸å¯è¯

æ­¤é¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ã€‚è¯¦æƒ…è¯·å‚é˜… LICENSE æ–‡ä»¶ã€‚

## è´¡çŒ®

æ¬¢è¿æäº¤é—®é¢˜å’Œæ‹‰å–è¯·æ±‚ã€‚åœ¨è´¡çŒ®ä¹‹å‰ï¼Œè¯·å…ˆé˜…è¯»è´¡çŒ®æŒ‡å—ã€‚

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·åˆ›å»º GitHub Issue æˆ–è”ç³»ç»´æŠ¤è€…ã€‚

---

## English

An LLM-based quantitative encyclopedia document processing tool, specifically designed for batch processing and translation of markdown documents.

## Features

- ğŸš€ **Efficient Batch Processing**: Supports multi-threaded parallel processing for maximum efficiency
- ğŸ”§ **Model Compatibility**: Supports OpenAI-compatible API interfaces, can integrate with various large language models
- ğŸ“ **Smart Editing**: YAML-based prompt templates for flexible processing logic customization
- ğŸŒ **Document Translation**: Optimized Chinese translation and formatting functionality
- ğŸ“ **Structure Preservation**: Automatically maintains original folder structure, copies non-markdown files

## Project Structure

```
quant-wiki-agent/
â”œâ”€â”€ llm_utils/          # LLM utility modules
â”‚   â””â”€â”€ llm_client.py   # Large language model client
â”œâ”€â”€ prompts/            # Prompt templates
â”‚   â”œâ”€â”€ complete_library.yaml
â”‚   â”œâ”€â”€ edit_prompt.yaml
â”‚   â””â”€â”€ re-translate.yaml
â”œâ”€â”€ test/               # Tests and examples
â”‚   â”œâ”€â”€ example_usage.py
â”‚   â”œâ”€â”€ folder_processor.py
â”‚   â””â”€â”€ single_file_test.py
â””â”€â”€ README.md
```

## Main Features

### 1. LLM Client (`llm_utils/llm_client.py`)

Provides unified large language model calling interface, supporting:
- OpenAI API
- Gemini API (via OpenAI-compatible interface)
- Other OpenAI API-compatible model services

**Key Features:**
- Automatic retry mechanism
- Environment variable configuration
- System prompt settings
- Error handling and logging

### 2. Batch Document Processing (`test/folder_processor.py`)

Core functionality module supporting:
- Multi-threaded parallel processing of markdown files
- Automatic copying of non-markdown files
- Maintains original folder structure
- Progress monitoring and status reporting

**Processing Flow:**
1. Scan input folder, identify markdown files
2. Copy all non-markdown files to output directory
3. Multi-threaded parallel processing of markdown files
4. Generate processing report

### 3. Prompt Templates (`prompts/`)

Provides professional document processing prompts:
- `re-translate.yaml`: Specialized for re-translating and formatting Chinese documents
- `edit_prompt.yaml`: General document editing prompts
- `complete_library.yaml`: Complete document library processing template

## Installation and Configuration

### 1. Requirements

- Python 3.7+
- Dependencies:
  ```bash
  pip install openai python-dotenv
  ```

### 2. Environment Configuration

Create `.env` file and configure the following variables:

```env
# Model configuration
MODEL_NAME=gpt-4o
API_KEY=your_api_key_here
BASE_URL=https://api.openai.com/v1/

# System prompt (optional)
DEFAULT_SYSTEM_PROMPT=You are a professional document processing assistant.
```

### 3. Supported Model Configurations

#### OpenAI Models
```env
MODEL_NAME=gpt-4o
API_KEY=your_openai_api_key
BASE_URL=https://api.openai.com/v1/
```

#### Gemini Models
```env
MODEL_NAME=gemini-1.5-pro-latest
API_KEY=your_gemini_api_key
BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai/
```

## Usage

### 1. Basic Usage

```python
from test.folder_processor import process_folder

# Process folder
result = process_folder(
    input_folder_path="/path/to/input",
    output_folder_path="/path/to/output",
    prompt_file_path="/path/to/prompts/re-translate.yaml",
    max_workers=30,
    rate_limit_delay=0.5
)

print(f"Processing complete! Success: {result['successful_count']}, Failed: {result['failed_count']}")
```

### 2. Using LLM Client Separately

```python
from llm_utils.llm_client import LLMClient

# Initialize client
client = LLMClient(
    model_name="gpt-4o",
    api_key="your_api_key",
    system_prompt="You are a professional translation assistant."
)

# Call model
response = client.call("Please translate this text")
print(response)
```

### 3. Running Examples

```bash
cd test
python example_usage.py
```

## Configuration Parameters

### Folder Processor Parameters

- `input_folder_path`: Input folder path
- `output_folder_path`: Output folder path
- `prompt_file_path`: Prompt file path (optional, defaults to edit_prompt.yaml)
- `max_workers`: Maximum concurrent threads (default 30)
- `rate_limit_delay`: API call interval in seconds (default 0.5 seconds)

### LLM Client Parameters

- `model_name`: Model name
- `api_key`: API key
- `base_url`: API base URL
- `system_prompt`: System prompt
- `max_retries`: Maximum retry attempts (default 3)
- `retry_delay`: Retry interval (default 1 second)

## Special Features

### Document Formatting and Translation

Specialized for quantitative finance document processing, supporting:
- Remove redundant metadata fields
- Clean hyperlink formats
- Unify markdown formatting
- Chinese-English translation
- Preserve code blocks and mathematical formulas

### Smart Error Handling

- Automatic retry mechanism
- Detailed error logging
- Processing progress monitoring
- Failed file statistics

### Performance Optimization

- Multi-threaded parallel processing
- Configurable API call frequency limits
- Memory-friendly streaming processing
- Automatic copying of non-processed files

## Notes

1. **API Quota Management**: Control concurrency and call frequency to avoid exceeding API limits
2. **File Backup**: Backup original files before processing
3. **Model Selection**: Choose appropriate models based on document complexity
4. **Network Stability**: Ensure stable network connection to avoid processing interruptions

## License

This project uses the MIT License. See the LICENSE file for details.

## Contributing

Issues and pull requests are welcome. Please read the contribution guidelines before contributing.

## Contact

For questions or suggestions, please create a GitHub Issue or contact the maintainer.